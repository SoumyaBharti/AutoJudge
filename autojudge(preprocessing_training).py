# -*- coding: utf-8 -*-
"""AutoJudge(preprocessing_training).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1muq6IVQoDIkftQ2F6Y-1gKPd909Nscvy
"""

!pip install scikit-learn pandas numpy matplotlib seaborn streamlit nltk

import pandas as pd

df = pd.read_json("/content/problems_data.jsonl", lines=True)
df.head()

df.shape

df.columns

df.dtypes

df.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='problem_class', data=df)
plt.title("Distribution of Problem Difficulty Classes")
plt.show()

plt.figure(figsize=(8,4))
sns.histplot(df['problem_score'], bins=30, kde=True)
plt.title("Distribution of Problem Difficulty Scores")
plt.show()

sns.boxplot(x=df['problem_score'])
plt.title("Problem Score Outliers")
plt.show()

sns.boxplot(x='problem_class', y='problem_score', data=df)
plt.title("Problem Score vs Difficulty Class")
plt.show()

text_cols = ['title', 'description', 'input_description', 'output_description']

for col in text_cols:
    if col in df.columns:
        df[col + '_len'] = df[col].astype(str).apply(len)

sns.boxplot(x='problem_class', y='description_len', data=df)
plt.title("Description Length vs Difficulty")
plt.show()

from sklearn.feature_extraction.text import CountVectorizer

hard_text = df[df['problem_class']=='hard']['description']

cv = CountVectorizer(stop_words='english', max_features=20)
X = cv.fit_transform(hard_text)

cv.get_feature_names_out()

df['problem_class'] = df['problem_class'].str.lower()

def stringify_sample_io(sample_io):
    if isinstance(sample_io, list):
        parts = []
        for item in sample_io:
            if isinstance(item, dict):
                for v in item.values():
                    parts.append(str(v))
            else:
                parts.append(str(item))
        return " ".join(parts)
    elif isinstance(sample_io, dict):
        return " ".join(str(v) for v in sample_io.values())
    else:
        return str(sample_io)

def combine_text(row):
    return " ".join([
        str(row['title']),
        str(row['description']),
        str(row['input_description']),
        str(row['output_description']),
        stringify_sample_io(row['sample_io'])
    ])

df['full_text'] = df.apply(combine_text, axis=1)

df['full_text'].head(2)

df['full_text'].str.len().describe()

df['text_len'] = df['full_text'].str.len()

symbols = ['+', '-', '*', '/', '=', '<', '>', '%']

def count_symbols(text):
    return sum(text.count(s) for s in symbols)

df['symbol_count'] = df['full_text'].apply(count_symbols)

import re
import numpy as np
df["line_count"] = df["full_text"].str.count("\n")
df["constraint_mentions"] = df["full_text"].str.contains("constraint|limit|bound", case=False).astype(int)
df["dp_keyword"] = df["full_text"].str.contains("dynamic programming|dp", case=False).astype(int)
df["graph_keyword"] = df["full_text"].str.contains("graph|tree|dfs|bfs", case=False).astype(int)
df['greedy_keyword'] = df['full_text'].str.lower().str.contains('greedy|optimal', case=False).astype(int)
df['string_keyword'] = df['full_text'].str.lower().str.contains('string|substring|character', case=False).astype(int)
df['array_keyword'] = df['full_text'].str.lower().str.contains('array|sequence|list', case=False).astype(int)

df['word_count'] = df['full_text'].str.split().str.len()
df['optimal_mentions'] = df['full_text'].str.lower().str.contains('optimal|minimum|maximum').astype(int)

data_structures = ['array', 'tree', 'graph', 'stack', 'queue', 'heap', 'hash', 'matrix']
df['data_structure_count'] = df['full_text'].apply(
    lambda x: sum(len(re.findall(rf'\b{ds}\b', x.lower())) for ds in data_structures)
)

df['number_count'] = df['full_text'].apply(lambda x: len(re.findall(r'\d+', x)))

df['sentence_count'] = df['full_text'].apply(
    lambda x: len([s for s in re.split(r'[.!?]+', x) if s.strip()])
)

df['avg_word_length'] = df['full_text'].apply(
    lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0
)
df['big_o_mentioned'] = df['full_text'].str.contains(r'O\(', case=False).astype(int)
df['class_encoded'] = df['problem_class'].map({
    'easy': 1,
    'medium': 2,
    'hard': 3
})

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(
    max_features=500,
    ngram_range=(1,2),
    stop_words='english',
    min_df=3,
    max_df=0.8,
    sublinear_tf=True
)

hf =["text_len", "symbol_count", "line_count",
     "constraint_mentions", "dp_keyword", "graph_keyword","greedy_keyword","string_keyword","array_keyword","word_count","optimal_mentions","data_structure_count","number_count","sentence_count","avg_word_length","big_o_mentioned"]
X_hand = df[hf].values

y_class = df['problem_class']
y_score = df['problem_score']

from sklearn.model_selection import train_test_split

X_text = df['full_text']
X_hand = X_hand
y_class = y_class
y_score = y_score

X_text_train, X_text_test, \
X_hand_train, X_hand_test, \
y_class_train, y_class_test, \
y_score_train, y_score_test = train_test_split(
    X_text,
    X_hand,
    y_class,
    y_score,
    test_size=0.2,
    random_state=42,
    stratify=y_class
)

X_train_tfidf = tfidf.fit_transform(X_text_train)
X_test_tfidf  = tfidf.transform(X_text_test)

y_class_train.value_counts(normalize=True)
y_class_test.value_counts(normalize=True)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_hand = scaler.fit_transform(X_hand_train)
X_test_hand = scaler.transform(X_hand_test)

from scipy.sparse import hstack

X_train = hstack([X_train_tfidf, X_train_hand])
X_test = hstack([X_test_tfidf, X_test_hand])

import numpy as np

X_train_gb = np.hstack([
    X_train_tfidf.toarray(),
    X_train_hand
])

X_test_gb = np.hstack([
    X_test_tfidf.toarray(),
    X_test_hand
])

from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

models = {
    'Logistic Regression': LogisticRegression(
        max_iter=2000,
        class_weight='balanced',
        n_jobs=-1,
        random_state=42
    ),
    'LinearSVC': LinearSVC(
        class_weight='balanced',
        max_iter=10000,
        random_state=42
    ),
    'Random Forest': RandomForestClassifier(
        n_estimators=200,
        max_depth=20,
        min_samples_split=5,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    ),
    'Gradient Boosting': HistGradientBoostingClassifier(
        max_iter=300,
        learning_rate=0.1,
        max_depth=5,
        random_state=42
    )
}

results = {}
trained_models = {}

for name, model in models.items():
    print(f"\n{'='*80}")
    print(f"Training: {name}")
    print(f"{'='*80}")

    if name == 'Gradient Boosting':
        model.fit(X_train_gb, y_class_train)
        y_pred = model.predict(X_test_gb)
    else:
        # Train
        model.fit(X_train, y_class_train)
        # Predict
        y_pred = model.predict(X_test)

    # Evaluate
    acc = accuracy_score(y_class_test, y_pred)
    results[name] = acc
    trained_models[name] = model

    print(f"\nAccuracy: {acc:.4f} ({acc*100:.2f}%)")
    print("\nClassification Report:")
    print(classification_report(y_class_test, y_pred))
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_class_test, y_pred))

best_model_name = max(results, key=results.get)
best_model = trained_models[best_model_name]
print(f"\nüèÜ Best Model: {best_model_name} with {results[best_model_name]*100:.2f}% accuracy")

rf_model = models['Random Forest']

# Get feature names
tfidf_features = tfidf.get_feature_names_out()
all_features = list(tfidf_features) + hf

# Get importances
importances = rf_model.feature_importances_

# Sort by importance
indices = np.argsort(importances)[::-1][:20]

print("\nTop 20 Most Important Features:")
for i, idx in enumerate(indices, 1):
    print(f"{i:2d}. {all_features[idx]:30s}: {importances[idx]:.4f}")

from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score

from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge

# Try multiple models with better hyperparameters
models_reg = {
    'HistGradientBoosting': HistGradientBoostingRegressor(
        max_depth=6,
        learning_rate=0.03,
        max_iter=600,
        min_samples_leaf=30,
        random_state=42
    ),
    'Random Forest': RandomForestRegressor(
        n_estimators=200,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    ),
    'Gradient Boosting': GradientBoostingRegressor(
        n_estimators=200,
        learning_rate=0.05,
        max_depth=7,
        min_samples_split=10,
        min_samples_leaf=5,
        subsample=0.8,
        random_state=42
    ),
    'Ridge Regression': Ridge(alpha=1.0, random_state=42)
}

results_reg = {}
trained_models_reg = {}

for name, model in models_reg.items():
    print(f"\n{'='*80}")
    print(f"Training: {name}")
    print(f"{'='*80}")

    # Train
    model.fit(X_train_gb, y_score_train)

    # Predict
    y_pred_train = model.predict(X_train_gb)
    y_pred_test = model.predict(X_test_gb)

    # Evaluate
    train_mae = mean_absolute_error(y_score_train, y_pred_train)
    test_mae = mean_absolute_error(y_score_test, y_pred_test)
    train_rmse = np.sqrt(mean_squared_error(y_score_train, y_pred_train))
    test_rmse = np.sqrt(mean_squared_error(y_score_test, y_pred_test))
    train_r2 = r2_score(y_score_train, y_pred_train)
    test_r2 = r2_score(y_score_test, y_pred_test)

    results_reg[name] = {
        'train_mae': train_mae,
        'test_mae': test_mae,
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'predictions': y_pred_test
    }
    trained_models_reg[name] = model

    print(f"\nTrain MAE:  {train_mae:.4f} | Test MAE:  {test_mae:.4f}")
    print(f"Train RMSE: {train_rmse:.4f} | Test RMSE: {test_rmse:.4f}")
    print(f"Train R¬≤:   {train_r2:.4f} | Test R¬≤:   {test_r2:.4f}")

best_reg_name = max(
    results_reg,
    key=lambda name: results_reg[name]['test_r2']
)

best_reg_model = trained_models_reg[best_reg_name]

print(
    f"Best Regressor: {best_reg_name} | "
    f"Test R¬≤ = {results_reg[best_reg_name]['test_r2']:.4f}"
)

import joblib

joblib.dump(tfidf, "tfidf.pkl")
joblib.dump(best_reg_model, "best_regressor.pkl")
joblib.dump(best_model, "best_classifier.pkl")
joblib.dump(scaler, "scaler.pkl")

